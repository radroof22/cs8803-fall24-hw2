{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Part 2 - Model Based Reinforcement Learning\n",
    "\n",
    "***\n",
    "\n",
    "Written by Albert Wilcox\n",
    "\n",
    "In this homework, you'll implement [PETS](https://arxiv.org/abs/1805.12114), a popular model for simple MBRL tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import einops\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "from src.utils import (\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    demo_policy,\n",
    "    save_frames_as_gif\n",
    ")\n",
    "# Do not remove the following import\n",
    "import src.cartpole_env\n",
    "from src.mpc import MPC\n",
    "from src.mbrl_utils import sample_rollout\n",
    "from src.cartpole_env import CartpoleConfigModule\n",
    "from src.mbrl_sampler import MBRLSampler\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED: int = 42\n",
    "ENVIRONMENT_NAME: str='MBRLCartpole-v0'\n",
    "\n",
    "# torch related defaults\n",
    "DEVICE = get_device()\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# Use random seeds for reproducibility\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we start by initializing the environment and printing some useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENVIRONMENT_NAME)\n",
    "\n",
    "# get the state and action dimensions\n",
    "action_dimension = env.action_space.shape[0]\n",
    "state_dimension = env.observation_space.shape[0]\n",
    "\n",
    "logger.info(f'Action Dimension: {action_dimension}')\n",
    "logger.info(f'Action High: {env.action_space.high}')\n",
    "logger.info(f'Action Low: {env.action_space.low}')\n",
    "logger.info(f'State Dimension: {state_dimension}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - PETS\n",
    "\n",
    "In this part you'll implement the PETS (Chua et al.) dynamics model and use it for model-based control (MPC). There are several important components of this pipeline:\n",
    " * The dynamics model, discussed in more detail in Chua et al, learns to predict the next state $s_{t+1}$ conditioned on the current state-action pair $(s_t, a_t)$.\n",
    " * The cost function outputs the cost of a planned state. In the case of this environment, we provide a ground truth cost function (negative velocity), but in more complicated environments where no ground truth cost function is available it is common to learn it.\n",
    " * Cross entropy method (CEM) is a gradient-free evolutionary optimizer. We use it to optimize sequences of actions, and evaluate these sequences of actions by predicting future states after rolling out the planned actions and computing the total cost of the predicted rollout under the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to set up our probabilistic dynamics model. As described in Chua et al, this should take in a state and action and output `mean` and `log_std` for a Gaussian distribution over possible future states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.networks import network\n",
    "\n",
    "class DynamicsModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 state_dimension: int, \n",
    "                 action_dimension: int,\n",
    "                 min_log_std: float = -5,\n",
    "                 max_log_std: float = 1,\n",
    "                 ):\n",
    "        super(DynamicsModel, self).__init__()\n",
    "        \n",
    "        # TODO: fill in the parameters to initialize the prediction network\n",
    "        \n",
    "    \n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the dynamics network. Should return mean and log_std of the next state distribution\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The input state.\n",
    "            action (torch.Tensor): The input action.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: The tuple (mean, log_std) of the distribution\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: predict the mean and log_std of the next state distribution as described above\n",
    "\n",
    "        return mean, log_std\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create an ensemble of dynamics models. There are better ways to implement this, but for the purposes of this assignment we'll simply maintain a list of models and loop through them at inference time. If you have access to a GPU and are interested in speeding up your implementation, you might want to check out https://pytorch.org/tutorials/intermediate/ensembling.html.\n",
    "\n",
    "There are several ways to handle data for ensembles, such as partitioning the dataset or training each network on different minibatches from the same dataset. For the purposes of this homework we'll use the easiest option, which is to train each model on the same minibatches. Due to the difference in randomly initialized NN weights, this still provides enough variance across models to give us some benefits.\n",
    "\n",
    "TODOs for this section:\n",
    " * Fill in the `forward` function of the dynamics model to predict the mean and log_std from each member of the ensemble for a single batch of states and actions. This is for use during training\n",
    " * Fill in the `compute_cost` function which takes in a single state and a batch of action trajectory candidates and computes the expected cost for each one by rolling out the dynamics model. You should do this using the TS-1 algorithm from Chua et al, meaning for each step you randomly sample a dynamics model from the ensemble. Note the parameter `n_particles`. For each action trajectory candidate, you should sample `n_particles` trajectories and compute the mean between their costs.\n",
    " * Note: the `compute_cost` function will involve creating some large tensors. For tensors with many dimensions to keep track of, I would highly recommend using the `einops` library for rearranging / tiling / etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleDynamicsModel(nn.Module):\n",
    "    def __init__(self, state_dimension: int, action_dimension: int, n_ensemble: int):\n",
    "        super(EnsembleDynamicsModel, self).__init__()\n",
    "        self.num_nets = n_ensemble\n",
    "\n",
    "        # TODO: initialize an ensemble of dynamics models\n",
    "        # Hint: You should store the models in an nn.ModuleList so that they appear when we do dynamics_model.parameters()\n",
    "    \n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the dynamics network. Should return mean and log_std of the next state distribution for each model in the ensemble\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The input state, shape (B, n_ensemble, S)\n",
    "            action (torch.Tensor): The input action, shape (B, n_ensemble, A)\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: The tuple (mean, log_std) of the distributions where each have shape (B, n_ensemble, S)\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: predict the next state as described above\n",
    "\n",
    "        return mean, log_std\n",
    "        \n",
    "    def compute_cost(\n",
    "            self, \n",
    "            state: torch.Tensor, \n",
    "            actions: torch.Tensor,\n",
    "            obs_cost_fn,\n",
    "            act_cost_fn,\n",
    "            n_particles: int = 20,\n",
    "        ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Given a state and a \n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The input state, shape (S,)\n",
    "            actions (torch.Tensor): The action sequence candidates, shape (N, H, A)\n",
    "            obs_cost_fn: A function which takes in a batch of states and returns the cost of each one\n",
    "            act_cost_fn: A function which takes in a batch of actions and returns the cost of each one\n",
    "            n_particles (int): how many particles to sample for each action sequence\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Expected cost for each action candidate, shape (N,)\n",
    "        \"\"\"\n",
    "        n_candidates, horizon, _ = actions.shape\n",
    "\n",
    "        # TODO: predict the trajectory using the TS-1 algorithm from Chua et al\n",
    "        # Hint: You may have issues with NaN values. To deal with this, use the reparameterization trick\n",
    "        #       to sample and then replace NaN costs with a high number\n",
    "\n",
    "        \n",
    "        return costs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've set up everything, the last step is to train our model. In the following block we provide some hyperparameters, the ground truth cost functions, and the skeleton of the training loop. You'll need to implement the loss function yourself.\n",
    "\n",
    "Unfortunately MPC is quite slow to run, especially on a CPU. Thus, we've provided you an offline dataset so that you don't need to run the MPC policy to collect online data. The dataset should be sufficient to achieve a reward greater than 150 with a correct implementation.\n",
    "\n",
    "Note: Our implementation achieved a validation loss <0.1.\n",
    "\n",
    "The hyperparameters we provide should work well enough, but if you have access to a GPU you can improve performance by increasing `n_particles`, `popsize` and `num_elites`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Hyper-parameters #########################################\n",
    "\n",
    "EPOCHS = 150\n",
    "EVAL_FREQ = 30\n",
    "TASK_HORIZON = 200\n",
    "\n",
    "plan_hor = 25\n",
    "n_particles = 10\n",
    "batch_size = 32\n",
    "n_ensemble = 5\n",
    "maxiters = 5\n",
    "popsize = 100\n",
    "num_elites = 10\n",
    "\n",
    "################################### Cost Functions ###########################################\n",
    "\n",
    "sampler = MBRLSampler(torch.load('data.pkl'), n_ensemble, batch_size, DEVICE)\n",
    "\n",
    "# To make things faster for you we're providing an offline dataset that should be sufficient\n",
    "rollouts = torch.load('data.pkl')\n",
    "all_obs = np.concatenate([rollout['obs'] for rollout in rollouts], axis=0)\n",
    "all_act = np.concatenate([rollout['act'] for rollout in rollouts], axis=0)\n",
    "all_next_obs = np.concatenate([rollout['next_obs'] for rollout in rollouts], axis=0)\n",
    "\n",
    "config = CartpoleConfigModule(DEVICE)\n",
    "dynamics_model = EnsembleDynamicsModel(state_dimension, action_dimension, n_ensemble).to(DEVICE)\n",
    "optimizer = Adam(dynamics_model.parameters(), 1e-3, weight_decay=1e-4)\n",
    "policy = MPC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    obs_cost_fn=config.obs_cost_fn,\n",
    "    act_cost_fn=config.ac_cost_fn,\n",
    "    dynamics_model=dynamics_model,\n",
    "    plan_hor=plan_hor,\n",
    "    n_particles=n_particles,\n",
    "    max_iters=maxiters,\n",
    "    popsize=popsize,\n",
    "    num_elites=num_elites,\n",
    "    alpha=0.1,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "data_len = all_obs.shape[0]\n",
    "\n",
    "epoch_range = trange(EPOCHS, unit=\"epoch(s)\", desc=\"Network training\")\n",
    "num_batch = int(np.ceil(data_len / batch_size))\n",
    "result = None\n",
    "rews = []\n",
    "\n",
    "for epoch in epoch_range:\n",
    "\n",
    "    for obs, act, next_obs in sampler:\n",
    "\n",
    "        # TODO: compute NLL loss and update the dynamics model\n",
    "        pass\n",
    "\n",
    "    # Compute validation MSE loss\n",
    "    # Note: this is a different loss function than the one you should implement to update the model\n",
    "    val_obs, val_act, val_next_obs = sampler.get_val_data()\n",
    "    mean, _ = dynamics_model(val_obs, val_act)\n",
    "    mse_losses = ((mean - val_next_obs) ** 2).mean()\n",
    "\n",
    "    epoch_range.set_postfix({\n",
    "        \"Training loss\": mse_losses.item(),\n",
    "        'Reward': result\n",
    "    })\n",
    "\n",
    "    # Sample an eval rollout. Note: If you are not using a GPU you should comment this out and only run eval once\n",
    "    if (epoch + 1) % EVAL_FREQ == 0:\n",
    "        info = sample_rollout(\n",
    "            env,\n",
    "            TASK_HORIZON,\n",
    "            policy=policy,\n",
    "        )\n",
    "        result = info['reward_sum']\n",
    "        rews.append(result)\n",
    "\n",
    "torch.save(dynamics_model.state_dict(), 'pets_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've finished training your dynamics model we can visualize our MPC policy and print out the final reward.\n",
    "\n",
    "Note: If you're running this on a CPU it will likely be quite slow. I would suggest visualizing a much shorter episode and making sure that the policy looks right before running the full 200 step eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics_model.load_state_dict(torch.load('pets_checkpoint.pth'))\n",
    "dynamics_model = dynamics_model.to(DEVICE)\n",
    "frames, total_reward = demo_policy(policy, environment_name=ENVIRONMENT_NAME, steps=200)\n",
    "gif_path = save_frames_as_gif(frames, method_name='pets')\n",
    "print('Total Reward:', total_reward)\n",
    "Image(open(gif_path,'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on finishing the MBRL portion of Assignment 2! Hopefully you enjoyed yourself. Make sure that the visualizations are showing, an eval with a success rate greater than 150 is showing above, and that the `pets_policy.gif` is present in the outputs folder.\n",
    "\n",
    "When you're done: export this notebook as an **HTML file** for final submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
