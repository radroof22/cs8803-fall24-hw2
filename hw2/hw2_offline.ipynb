{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Part 1 - Behavior Cloning and Offline RL\n",
    "\n",
    "***\n",
    "\n",
    "Written by Albert Wilcox\n",
    "\n",
    "In this homework, you'll implement DAgger and Implicit Q learning on the `halfcheetah-medium-replay-v2` task from the [D4RL benchmark](https://github.com/Farama-Foundation/D4RL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from tqdm import tqdm\n",
    "import einops\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from src.utils import (\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    eval_policy,\n",
    "    demo_policy,\n",
    "    plot_returns,\n",
    "    save_frames_as_gif,\n",
    "    update_exponential_moving_average,\n",
    "    return_range\n",
    ")\n",
    "from src.d4rl_dataset import D4RLSampler\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0 - Setting up D4RL and Dataset\n",
    "\n",
    "The first step for training on the D4RL benchmark is to set up the environment. Unfortunately mujoco can be difficult to install. Run the following block to test your install. If you have any issues, Google is your friend :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d4rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to initialize our environment and set the random seed for a variety of libraries in order to ensure determinism in our experiments.\n",
    "\n",
    "For this homework we'll be using the `halfcheetah-medium-replay-v2` environment. This environment involves training a two-legged cheetah to run, and comes with a dataset that consists of data from rolling out a suboptimal SAC agent and exploration data from the SAC replay buffer. Thus, it comes from a wide distribution of policies and contains a good amount of suboptimal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED: int = 42\n",
    "ENVIRONMENT_NAME: str='halfcheetah-medium-replay-v2'\n",
    "\n",
    "# torch related defaults\n",
    "DEVICE = get_device()\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# Use random seeds for reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# instantiate the environment\n",
    "env = gym.make(ENVIRONMENT_NAME)\n",
    "\n",
    "# get the state and action dimensions\n",
    "action_dimension = env.action_space.shape[0]\n",
    "state_dimension = env.observation_space.shape[0]\n",
    "\n",
    "logger.info(f'Action Dimension: {action_dimension}')\n",
    "logger.info(f'Action High: {env.action_space.high}')\n",
    "logger.info(f'Action Low: {env.action_space.low}')\n",
    "logger.info(f'State Dimension: {state_dimension}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a dataset. Luckily for us, D4RL provides datasets that are convenient to download and train on. Running the following command should download and cache the dataset and initialize a dataset object before printing out some useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = d4rl.qlearning_dataset(env)\n",
    "\n",
    "logger.info(f'Dataset type: {type(dataset)}')\n",
    "logger.info(f'Dataset keys: {dataset.keys()}')\n",
    "logger.info(f'# Samples: {len(dataset[\"observations\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we wrap the D4RL dataset in a sampler. You can comment out the lines at the bottom to make sure everything runs smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampler = D4RLSampler(dataset, 256, DEVICE)\n",
    "\n",
    "# Uncomment the following lines to iterate through the datset and make sure everything runs smoothly\n",
    "# for _ in tqdm(sampler):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Behavior Cloning\n",
    "\n",
    "In this part of the homework you'll implement behavior cloning.\n",
    "\n",
    "Next, train a BC agent by minimizing the negative log likelihood (NLL) of the predicted distribution on datset actions.\n",
    "\n",
    "You should achieve a maximum normalized reward greater than 0.35 with the provided hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.networks import GaussianPolicy\n",
    "\n",
    "################################## Hyper-parameters #########################################\n",
    "\n",
    "EPOCHS: int = 50\n",
    "EVAL_FREQ = 5\n",
    "LOAD_CKPT = False\n",
    "\n",
    "hidden_dim: int = 256\n",
    "n_hidden: int = 3\n",
    "lr: float = 3e-4\n",
    "WEIGHT_DECAY: float = 3e-4\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "bc_policy = GaussianPolicy(state_dimension, action_dimension, hidden_dim, n_hidden).to(DEVICE)\n",
    "optimizer = Adam(bc_policy.parameters(), lr)\n",
    "\n",
    "if LOAD_CKPT and os.path.exists('bc_policy.pth'):\n",
    "    ckpt = torch.load('bc_policy.pth')\n",
    "    bc_policy.load_state_dict(ckpt['state_dict'])\n",
    "    means = ckpt['means']\n",
    "    stds = ckpt['stds']\n",
    "else:\n",
    "    means, stds = [], []\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(sampler):\n",
    "            state = batch['state'].to(DEVICE)\n",
    "            action = batch['action'].to(DEVICE)\n",
    "            \n",
    "            # TODO: compute negative log likelihood loss on this batch\n",
    "            loss = torch.tensor(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % EVAL_FREQ == 0:\n",
    "            rew_mean, rew_std = eval_policy(bc_policy, environment_name=ENVIRONMENT_NAME, eval_episodes=50)\n",
    "            logger.info(f'Epoch: {epoch + 1}. Loss: {total_loss / len(sampler):.4f}. Reward: {rew_mean:.4f} +/- {rew_std:.4f}')\n",
    "            means.append(rew_mean)\n",
    "            stds.append(rew_std)\n",
    "    # Save the policy and learning curve in case there is an issue so you can plot without retraining\n",
    "    exp_state = {\n",
    "        'state_dict': bc_policy.state_dict(),\n",
    "        'means': means,\n",
    "        'stds': stds\n",
    "    }\n",
    "    torch.save(exp_state, 'bc_policy.pth')\n",
    "epochs = np.arange(EVAL_FREQ, EPOCHS + EVAL_FREQ, step=EVAL_FREQ)\n",
    "plot_returns(means, stds,'Behavior Cloning', epochs=epochs, goal=0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've finished training, use the following block to visualize the policy you trained with BC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_policy.load_state_dict(torch.load('bc_policy.pth')['state_dict'])\n",
    "frames, total_reward = demo_policy(bc_policy, environment_name=ENVIRONMENT_NAME, steps=200)\n",
    "gif_path = save_frames_as_gif(frames, method_name='bc')\n",
    "Image(open(gif_path,'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Part 2 - DAgger\n",
    "\n",
    "BC is great at replicating supervisor actions when the agent is in the data distribution, but this assumption is not always true. Sometimes the agent may enter an out of distribution state and output bad actions. A popular method to handle this issue is [Dataset Aggregation (DAgger)](https://arxiv.org/abs/1011.0686). The key idea behind DAgger is to roll out the learned policy while querying an expert policy on the states the agent encounters, adding the state-expert action pairs to the dataset. \n",
    "\n",
    "Luckily for you, we're providing an expert pretrained using the Soft Actor Critic Algorithm, which we'll load and test in the following block. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Implicit Q Learning\n",
    "\n",
    "In this part you'll implement Implicit Q-Learning (Kostrikov et al., 2021), a popular offline RL algorithm. \n",
    "\n",
    "The key idea behind IQL is to use expectile regression to optimize the value functions so that they estimate the values of the higher-performing actions in the dataset, rather than estimating the values of the current policy. This allows you to learn a value function without ever querying the policy, which helps to avoid OOD issues. We would suggest having a look at Kostrikov et al. for a more thorough description of the algorithm.\n",
    "\n",
    "To start, implement a double Q function below. This can be similar to the code from HW1 but notice the constructor has a different signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.networks import network\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dimension, action_dimension, hidden_dim, n_hidden):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # TODO: fill in your code here\n",
    "\n",
    "    def forward(self, state, action):\n",
    "\n",
    "        # TODO: fill in your code here to query the critic\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement a value network below. This should be similar to the Q network, but only condition on states and should only have one network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNetwork(nn.Module):\n",
    "    def __init__(self, state_dimension, hidden_dim, n_hidden):\n",
    "        super(VNetwork, self).__init__()\n",
    "\n",
    "        # TODO: your code here\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        # TODO: your code here\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the expectile loss, $L^{\\tau}_2$, to be used for optimizing the value function. This function is described in Sections 4.1 of Kostrikov et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectile_loss(diff, expectile=0.8):\n",
    "    # TODO: fill in this function\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time to implement the IQL training loop. There are several steps here:\n",
    " * Implement the value function update using the `expectile_loss` function implemented above.\n",
    " * Implement the $Q$ function update. The targets for this update should be a bellman backup based on the value function. Don't forget to update the EMA target!\n",
    " * Implement the policy update. This should be an NLL loss weighted based on clipped exponentiated advantage estimates\n",
    "\n",
    "More details about all of these steps can be found in Kostrikov et al.\n",
    "\n",
    "Once you've finished implementing the training loop run the cell to train your IQL policy. Your policy should get reward greater than 0.43 with a correct implementation and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "EVAL_FREQ = 15\n",
    "LOAD_FROM_CKPT = False\n",
    "\n",
    "# These parameters should work fine but you may tune them if you want to\n",
    "hidden_dim: int = 256\n",
    "n_hidden: int = 2\n",
    "lr: float = 3e-4\n",
    "discount = 0.99\n",
    "alpha = 0.005\n",
    "exp_advantage_max = 100\n",
    "\n",
    "# TODO: you'll need to choose your own value for the following parameters\n",
    "tau = ???\n",
    "beta = ???\n",
    "\n",
    "min_rew, max_rew = return_range(dataset, 1000)\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "sampler = D4RLSampler(dataset, 256, DEVICE)\n",
    "\n",
    "iql_policy = GaussianPolicy(state_dimension, action_dimension, hidden_dim, n_hidden).to(DEVICE)\n",
    "policy_optimizer = Adam(iql_policy.parameters(), lr)\n",
    "policy_lr_schedule = CosineAnnealingLR(policy_optimizer, EPOCHS * len(sampler))\n",
    "\n",
    "v_critic = VNetwork(state_dimension, hidden_dim, n_hidden).to(DEVICE)\n",
    "v_optimizer = Adam(v_critic.parameters(), lr)\n",
    "\n",
    "q_critic = QNetwork(state_dimension, action_dimension, hidden_dim, n_hidden).to(DEVICE)\n",
    "q_critic_target = copy.deepcopy(q_critic)\n",
    "q_critic_target.requires_grad_(False)\n",
    "q_optimizer = Adam(q_critic.parameters(), lr)\n",
    "\n",
    "means, stds, start_epoch = [], [], 0\n",
    "if os.path.exists('iql_checkpoint.pth') and LOAD_FROM_CKPT:\n",
    "    checkpoint = torch.load('iql_checkpoint.pth')\n",
    "\n",
    "    iql_policy.load_state_dict(checkpoint['iql_policy'])\n",
    "    policy_optimizer.load_state_dict(checkpoint['policy_optimizer'])\n",
    "    v_critic.load_state_dict(checkpoint['v_critic'])\n",
    "    v_optimizer.load_state_dict(checkpoint['v_optimizer'])\n",
    "    q_critic.load_state_dict(checkpoint['q_critic'])\n",
    "    q_critic_target.load_state_dict(checkpoint['q_critic_target'])\n",
    "    q_optimizer.load_state_dict(checkpoint['q_optimizer'])\n",
    "    \n",
    "    start_epoch = checkpoint['epoch']\n",
    "    means = checkpoint['means']\n",
    "    stds = checkpoint['stds']\n",
    "    \n",
    "    print(f'Resuming run from epoch {start_epoch}')\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    total_q_loss = total_v_loss = total_policy_loss = count = 0\n",
    "    policy_losses = []\n",
    "    # for batch in tqdm(dataloader):\n",
    "    for batch in tqdm(sampler):\n",
    "        state = batch['state'].to(DEVICE)\n",
    "        next_state = batch['next_state'].to(DEVICE)\n",
    "        action = batch['action'].to(DEVICE)\n",
    "        reward = einops.rearrange(batch['reward'], 'b -> b 1').to(DEVICE)\n",
    "        reward = reward / (max_rew - min_rew) * 1000\n",
    "        not_done = einops.rearrange(batch['not_done'], 'b -> b 1').to(DEVICE)\n",
    "\n",
    "        # TODO: update the state value function (V)\n",
    "        v_loss = torch.tensor(0)\n",
    "\n",
    "        # TODO: update the state-action value function (Q) and the target\n",
    "        q_loss = torch.tensor(0)\n",
    "\n",
    "        # TODO: update the policy\n",
    "        policy_loss = torch.tensor(0)\n",
    "\n",
    "        policy_lr_schedule.step()\n",
    "        total_v_loss += v_loss.item()\n",
    "        total_q_loss += q_loss.item()\n",
    "        total_policy_loss += policy_loss.item()\n",
    "        count += 1\n",
    "        \n",
    "    if (epoch + 1) % EVAL_FREQ == 0:\n",
    "        rew_mean, rew_std = eval_policy(iql_policy, environment_name=ENVIRONMENT_NAME, eval_episodes=50)\n",
    "        print(f'Epoch: {epoch + 1}. Q Loss: {total_q_loss / count:.4f}. V Loss: {total_v_loss / count:.4f}. P Loss: {total_policy_loss / count:.4f}. Reward: {rew_mean:.4f} +/- {rew_std:.4f}')\n",
    "        means.append(rew_mean)\n",
    "        stds.append(rew_std)\n",
    "\n",
    "    # Save a checkpoint so that you can resume training if it crashes\n",
    "    checkpoint = {\n",
    "        'iql_policy': iql_policy.state_dict(),\n",
    "        'policy_optimizer': policy_optimizer.state_dict(),\n",
    "        'v_critic': v_critic.state_dict(),\n",
    "        'v_optimizer': v_optimizer.state_dict(),\n",
    "        'q_critic': q_critic.state_dict(),\n",
    "        'q_critic_target': q_critic_target.state_dict(),\n",
    "        'q_optimizer': q_optimizer.state_dict(),\n",
    "        'epoch': epoch + 1,\n",
    "        'means': means,\n",
    "        'stds': stds\n",
    "    }\n",
    "    torch.save(checkpoint, 'iql_checkpoint.pth')\n",
    "\n",
    "epochs = np.arange(EVAL_FREQ, EPOCHS + EVAL_FREQ, step=EVAL_FREQ)\n",
    "plot_returns(means, stds, 'Implicit Q Learning', goal=0.4, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've finished training our IQL policy we can visualize it in the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iql_policy.load_state_dict(torch.load('iql_checkpoint.pth')['iql_policy'])\n",
    "frames, total_reward = demo_policy(iql_policy, environment_name=ENVIRONMENT_NAME, steps=200)\n",
    "gif_path = save_frames_as_gif(frames, method_name='iql')\n",
    "Image(open(gif_path,'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on finishing the offline portion of Assignment 2! Hopefully you enjoyed yourself. Make sure that the visualizations are showing and that there are four outputs from this notebook in the artifacts folder:\n",
    " * `bc_policy.gif`\n",
    " * `iql_policy.gif`\n",
    " * `Behavior Cloning_returns.png`\n",
    " * `Implicit Q Learning_results.png`\n",
    "\n",
    "When you're done: export this notebook as an **HTML file** for final submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
